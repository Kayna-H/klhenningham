#!/usr/bin/env python
# coding: utf-8

# In[21]:


#Gibbs Sampling Dirichlet Mixture Model (GSDMM) is an “altered” LDA algorithm, showing great results on STTM tasks, 
#that makes the initial assumption: 1 topic ↔️1 document. The words within a document are generated using the same 
#unique topic, and not from a mixture of topics as it was in the original LDA.
import pandas as pd
import numpy as np

# set seed for reproducibility
np.random.seed(493)

import json 
import csv

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk import word_tokenize, sent_tokenize
ps = nltk.porter.PorterStemmer()

import unicodedata
import re

from gsdmm import MovieGroupProcess

# to print out all the outputs
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

# set display options
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.max_colwidth', None)


# In[22]:


# read data info a dataframe
df = pd.read_json('/Users/kaynadabaddest/Downloads/gsdmm-short-clustering/data/sortedData/json/Fall2019Test2BM.json')

# remove  null values
df = df.notnull()


# In[23]:


#text preprocessing - removing stop words, stemming tokens
def basic_clean(original):
    word = original.lower()
    word = unicodedata.normalize('NFKD', word)                                .encode('ascii', 'ignore')                                .decode('utf-8', 'ignore')
    word = re.sub(r"[^a-z0-9'\s]", '', word)
    word = word.replace('\n',' ')
    word = word.replace('\t',' ')
    return word
  
def remove_stopwords(original, extra_words=[], exclude_words=[]):
    stopword_list = stopwords.words('english')

    for word in extra_words:
        stopword_list.append(word)
    for word in exclude_words:
        stopword_list.remove(word)

    words = original.split()
    filtered_words = [w for w in words if w not in stopword_list]

    original_nostop = ' '.join(filtered_words)

    return original_nostop
 
def stem(original):
    ps = nltk.porter.PorterStemmer()
    stems = [ps.stem(word) for word in original.split()]
    original_stemmed = ' '.join(stems)
    return original_stemmed

#put cleaned data into a new array
docs = []
for sentence in df:
    words = word_tokenize(stem(remove_stopwords(basic_clean(sentence))))
    docs.append(words)
#check and make sure docs are cleaned properly


# In[24]:


#Cluster data where K is number of topics, alpha is 0.1 (default parameter), beta is 1 ( default)
#from original gitHub package:
# K: upper bound on the number of possible clusters. Typically many fewer
#alpha: controls the probability that a student will join a table that is currently empty When alpha is 0, no one will join an empty table.
#beta: controls the student's affinity for other students with similar interests. A low beta means
# that students desire to sit with students of similar interests. A high beta means they are less
# concerned with affinity and are more influenced by the popularity of a table
#not sure if i can leave this as sac instead of sac
#try modifying parameters
sac = MovieGroupProcess(K=15, alpha=0.1, beta=1, n_iters=30)

vocab = set(x for doc in docs for x in doc)
n_terms = len(vocab)

y = sac.fit(docs, n_terms)


# In[25]:


#print data
doc_count = np.array(sac.cluster_doc_count)
print('Number of documents per topic :', doc_count)


# In[26]:


#sort data by importance
top_index = doc_count.argsort()[-15:][::-1]
print('Most important clusters (by number of docs inside):', top_index)


# In[27]:


#find top words generated by clustering
def top_words(cluster_word_distribution, top_cluster, values):
    for cluster in top_cluster:
        sort_dicts =sorted(sac.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]
        print('Cluster %s : %s'%(cluster,sort_dicts))
        print('-'*120)


# In[28]:


# Show the top 7 words in term frequency for each cluster 
top_words(sac.cluster_word_distribution, top_index, 7)


# In[33]:


#rename topics
topic_dict = {}
topic_names = ['Topic #1',
               'Topic #2',
               'Topic #3',
               'Topic #4',
               'Topic #5',
               'Topic #6',
               'Topic #7',
               'Topic #8',
               'Topic #9',
               'Topic #10',
               'Topic #11',
               'Topic #12',
               'Topic #13',
               'Topic #14',
               'Topic #15'
              ]
for i, topic_num in enumerate(top_index):
    topic_dict[topic_num]=topic_names[i]


# In[34]:


#create a dataframe with topic names
def create_topics_dataframe(data_text=df,  sac=sac, threshold=0.3, topic_dict=topic_dict, stem_text=docs):
    result = pd.DataFrame(columns=['text', 'topic', 'stems'])
    for i, text in enumerate(data_text):
        result.at[i, 'text'] = text
        result.at[i, 'stems'] = stem_text[i]
        prob = sac.choose_best_label(stem_text[i])
        if prob[1] >= threshold:
            result.at[i, 'topic'] = topic_dict[prob[0]]
        else:
            result.at[i, 'topic'] = 'Other'
    return result


# In[35]:


#view distribution
dfx = create_topics_dataframe(data_text=df,  sac=sac, threshold=0.3, topic_dict=topic_dict, stem_text=docs)


# In[36]:


dfx.topic.value_counts(dropna=False)


# In[ ]:




